{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1bQA4mPyPr8zrBPD-NXoV6aupf7QUOfYB","authorship_tag":"ABX9TyPbnludQB0/0yay94UNXqH+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Scrape an online Social Media Site for Data. Use python to scrapeinformation\n","from twitter. Exploratory Data Analysis and visualization of Social Media Data."],"metadata":{"id":"geKu18UiaP01"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOXN06D1Wecy","collapsed":true},"outputs":[],"source":["from googleapiclient.discovery import build\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Set up API key and channel IDs\n","api_key = \"AIzaSyBivHpCtwzkJrQQT1S0CoJ78jf2rHOb4jo\"\n","channel_ids = [\n","    'UCMiJRAwDNSNzuYeN2uWa0pA',  # mrwhoistheboss\n","    'UCdPui8EYr_sX6q1xNXCRPXg',  # storror\n","    'UCj22tfcQrWG7EMEKS0qLeEg'   # Carryminati\n","]\n","\n","# Initialize YouTube API client\n","youtube = build('youtube', 'v3', developerKey=api_key)\n","\n","# Function to get channel statistics\n","def get_channel_stats(youtube, channel_ids):\n","    all_data = []\n","    request = youtube.channels().list(\n","        part='snippet,contentDetails,statistics',\n","        id=','.join(channel_ids)\n","    )\n","    response = request.execute()\n","\n","    for i in range(len(response['items'])):\n","        data = dict(\n","            channel_name=response['items'][i]['snippet']['title'],\n","            Subscribers=response['items'][i]['statistics']['subscriberCount'],\n","            views=response['items'][i]['statistics']['viewCount'],\n","            Total_videos=response['items'][i]['statistics']['videoCount']\n","        )\n","        all_data.append(data)\n","\n","    return all_data\n","\n","# Calling the function and creating DataFrame\n","channel_statistics = get_channel_stats(youtube, channel_ids)\n","channel_data = pd.DataFrame(channel_statistics)\n","\n","# Convert data types\n","channel_data['Subscribers'] = pd.to_numeric(channel_data['Subscribers'])\n","channel_data['views'] = pd.to_numeric(channel_data['views'])\n","channel_data['Total_videos'] = pd.to_numeric(channel_data['Total_videos'])\n","\n","# Plotting\n","sns.barplot(x='channel_name', y='Subscribers', data=channel_data)\n","sns.barplot(x='channel_name', y='views', data=channel_data)\n","sns.barplot(x='channel_name', y='Total_videos', data=channel_data)\n","plt.show()\n"]},{"cell_type":"markdown","source":["Develop Content (text, emoticons, image, audio, video) based social media\n","analytics model for business. (e.g., Content Based Analysis: Topic, Issue, Trend,\n","sentiment/opinion analysis, audio, video, image analytics)"],"metadata":{"id":"qxRRTBd6gtH5"}},{"cell_type":"code","source":["# Uninstall conflicting versions\n","!pip uninstall -y numpy scipy gensim textblob\n","\n","# Reinstall compatible versions\n","!pip install numpy==1.26.4 scipy==1.13.1 gensim==4.3.3 textblob\n"],"metadata":{"id":"-FY23ytSiaOn","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy\n","import scipy\n","import gensim\n","import textblob\n","\n","\n","print(\"All imports working correctly!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8lRqzrRis0N","executionInfo":{"status":"ok","timestamp":1752272450926,"user_tz":-330,"elapsed":17,"user":{"displayName":"Aryan Singh","userId":"11476742217229803552"}},"outputId":"71b44bb1-ade0-416d-eb89-6342278d23ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All imports working correctly!\n"]}]},{"cell_type":"code","source":["import nltk\n","\n","# Download everything needed\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')  # Needed for lemmatizer\n","nltk.download('averaged_perceptron_tagger')  # Sometimes needed for TextBlob\n","nltk.download('brown')  # Optional, used by TextBlob in some functions\n","nltk.download('punkt_tab')  # Fixes your specific issue\n"],"metadata":{"collapsed":true,"id":"KlXOi1SKlVGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","import gensim\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim import corpora, models\n","from textblob import TextBlob\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Load your data\n","path = \"/content/drive/MyDrive/Datasets/google.csv\"\n","df = pd.read_csv(path)\n","\n","# Extract non-empty reviews\n","reviews = df['Reviews'].dropna().tolist()\n","\n","# Display first few reviews\n","print(\"Sample Reviews:\\n\", reviews[:5])\n","\n","# Preprocessing function\n","def preprocess(text):\n","    # Tokenization\n","    tokens = word_tokenize(text.lower())\n","    # Remove stopwords and non-alphanumeric tokens\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n","    return lemmatized_tokens\n","\n","# Topic modeling\n","def topic_modeling(reviews):\n","    processed_reviews = [preprocess(review) for review in reviews]\n","    dictionary = corpora.Dictionary(processed_reviews)\n","    corpus = [dictionary.doc2bow(review) for review in processed_reviews]\n","    lda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n","    topics = lda_model.print_topics(num_words=3)\n","    return topics\n","\n","# Sentiment analysis\n","def sentiment_analysis(text):\n","    blob = TextBlob(text)\n","    polarity = blob.sentiment.polarity\n","    if polarity > 0:\n","        return \"Positive\"\n","    elif polarity < 0:\n","        return \"Negative\"\n","    else:\n","        return \"Neutral\"\n","\n","# Analyze reviews\n","def analyze_reviews(reviews):\n","    df_result = pd.DataFrame(columns=['Review', 'Sentiment'])\n","    for idx, review in enumerate(reviews, start=1):\n","        sentiment = sentiment_analysis(review)\n","        print(f\"\\nReview {idx}: {review}\")\n","        print(f\"Sentiment: {sentiment}\")\n","        df_result.loc[idx] = {'Review': review, 'Sentiment': sentiment}\n","    topics = topic_modeling(reviews)\n","    return df_result, topics\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    review_df, topics = analyze_reviews(reviews)\n","\n","    print(\"\\nSentiment Analysis Summary:\")\n","    print(review_df)\n","\n","    print(\"\\nTopics Identified by LDA:\")\n","    for idx, topic in enumerate(topics):\n","        print(f\"Topic {idx + 1}: {topic}\")\n"],"metadata":{"id":"FZ5al5OwgsVK","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Develop Structure based social media analytics model for any business.\n","(e.g. Structure Based Models -community detection influence analysis)"],"metadata":{"id":"XBDWH5uxmsTh"}},{"cell_type":"code","source":["import pandas as pd\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from networkx.algorithms import community\n","import community as community_louvain\n","\n","# Load the dataset\n","path =\"/content/drive/MyDrive/Datasets/fake_social_data_100.csv\"\n","\n","df = pd.read_csv(path)\n","# Display the first few rows of the dataframe\n","print(df.head())\n","# Create the graph from the edge list\n","fb_graph = nx.from_pandas_edgelist(df, source=\"id\", target=\"friend_id\")\n","# Display all the nodes\n","print(fb_graph.nodes())\n","# Display all the edges\n","print(fb_graph.edges())\n","# Add a new edge to the graph\n","fb_graph.add_edge(123, 2154)\n","# Display all the nodes again\n","print(fb_graph.nodes())\n","# Community detection using Girvan-Newman algorithm\n","comp = community.girvan_newman(fb_graph)\n","first_level_communities = next(comp)\n","second_level_communities = next(comp)\n","first_community_list = sorted(map(sorted, first_level_communities))\n","second_community_list = sorted(map(sorted, second_level_communities))\n","print(f\"First level communities: {first_community_list}\")\n","print(f\"Second level communities: {second_community_list}\")\n","# Community detection using Louvain method\n","partition = community_louvain.best_partition(fb_graph)\n","# Plot the communities detected by the Louvain method\n","pos = nx.spring_layout(fb_graph)\n","cmap = plt.get_cmap('viridis')\n","colors = [partition[node] for node in fb_graph.nodes()]\n","plt.figure(figsize=(12, 12))\n","nx.draw(fb_graph, pos, node_color=colors, with_labels=True, cmap=cmap, node_size=50,\n","font_size=8)\n","plt.show()\n","# Calculate degree centrality\n","degree_centrality = nx.degree_centrality(fb_graph)\n","# Sort and display the degree centrality values\n","sorted_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1],\n","reverse=True)\n","print(\"Degree Centrality:\", sorted_degree_centrality)\n","# Calculate betweenness centrality\n","betCent = nx.betweenness_centrality(fb_graph, normalized=True, endpoints=True)\n","sorted_betCent = sorted(betCent.items(), key=lambda x: x[1], reverse=True)\n","print(\"Betweenness Centrality:\", sorted_betCent)\n","# Plot the graph with betweenness centrality\n","node_color = [20000.0 * fb_graph.degree(v) for v in fb_graph]\n","node_size = [v * 10000 for v in betCent.values()]\n","plt.figure(figsize=(20, 20))\n","nx.draw_networkx(fb_graph, pos=pos, with_labels=False, node_color=node_color,\n","node_size=node_size)\n","plt.axis(\"off\")\n","plt.show()\n","\n","# Calculate and print closeness centrality\n","closeness_centrality = nx.closeness_centrality(fb_graph)\n","sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda item: item[1],\n","reverse=True)\n","print(\"Closeness Centrality:\", sorted_closeness_centrality[:8])\n","# Plot the graph with closeness centrality\n","node_size = [v * 50 for v in closeness_centrality.values()]\n","\n","plt.figure(figsize=(15, 8))\n","nx.draw_networkx(fb_graph, pos=pos, node_size=node_size, with_labels=False, width=0.15)\n","plt.axis(\"off\")\n","plt.show()\n","# Check for bridges in the graph\n","print(nx.has_bridges(fb_graph))\n","# Find and print all the bridges\n","bridges = list(nx.bridges(fb_graph))\n","print(\"Number of bridges:\", len(bridges))\n","# Find and print all the local bridges\n","local_bridges = list(nx.local_bridges(fb_graph, with_span=False))\n","print(\"Number of local bridges:\", len(local_bridges))\n","# Plot the graph highlighting the local bridges\n","plt.figure(figsize=(15, 5))\n","nx.draw_networkx(fb_graph, pos=pos, node_size=10, with_labels=False, width=0.15)\n","nx.draw_networkx_edges(fb_graph, pos, edgelist=local_bridges, width=0.5,\n","edge_color=\"green\")\n","plt.axis(\"off\")\n","plt.show()\n","# Calculate and print the average clustering coefficient\n","print(\"Average clustering coefficient:\", nx.average_clustering(fb_graph))"],"metadata":{"collapsed":true,"id":"L9a96H7SmtIT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use Graph Neural Networks on the datasets (Planetoid Cora Dataset)/ Jazz\n","Musicians Network"],"metadata":{"id":"s2l3AZ_nqrU5"}},{"cell_type":"code","source":["!pip install torch\n","!pip install torch-geometric\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.datasets import Planetoid\n","import torch_geometric.transforms as T\n","from torch_geometric.nn import GCNConv\n","dataset = Planetoid(root='./', name='Cora', transform=T.NormalizeFeatures())"],"metadata":{"collapsed":true,"id":"818SX8jwqtdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","\n","# Assuming dataset is already loaded\n","data = dataset[0]\n","\n","# Define the GCN model\n","class GCN(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super(GCN, self).__init__()\n","        self.conv1 = GCNConv(in_channels, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=1)\n","\n","# Initialize the model and optimizer\n","model = GCN(dataset.num_features, 16, dataset.num_classes)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","print(optimizer)\n","\n","# Training function\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data.x, data.edge_index)\n","    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()\n","\n","# Test function\n","def test():\n","    model.eval()\n","    logits = model(data.x, data.edge_index)\n","    accs = []\n","    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n","        pred = logits[mask].max(1)[1]\n","        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n","        accs.append(acc)\n","    return accs\n","\n","# Training loop\n","for epoch in range(200):\n","    loss = train()\n","    train_acc, val_acc, test_acc = test()\n","    if epoch % 10 == 0:\n","        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '\n","              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n","\n","# Final test accuracy\n","model.eval()\n","_, pred = model(data.x, data.edge_index).max(dim=1)\n","correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n","acc = correct / data.test_mask.sum().item()\n","print(f'Test Accuracy: {acc:.4f}')\n"],"metadata":{"id":"eUtzoPE8rdVf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pr 10) gephi code\n"],"metadata":{"id":"76MqbjC48bRS"}},{"cell_type":"code","source":["import pandas as pd\n","import networkx as nx\n","\n","# Load the CSV data (adjust path if needed)\n","df = pd.read_csv('/content/drive/MyDrive/Datasets/twitter_user_interactions.csv')\n","\n","# Initialize a directed graph\n","G = nx.DiGraph()\n","\n","# Add nodes with user data\n","for index, row in df.iterrows():\n","    G.add_node(\n","        row['user'],\n","        tweets=row['tweets'],\n","        retweets=row['retweets'],\n","        likes=row['likes'],\n","        mentions=row['mentions'],\n","        followers=row['followers']\n","    )\n","\n","# Create dummy edges: each user mentions the next user in the list\n","users = df['user'].tolist()\n","for i in range(len(users) - 1):\n","    G.add_edge(users[i], users[i + 1], weight=df.loc[i, 'mentions'])\n","\n","# âœ… Save the graph to a GEXF file in Colab\n","output_path = '/content/twitter_network.gexf'\n","nx.write_gexf(G, output_path)\n","\n","print(f\"Graph successfully saved to: {output_path}\")\n","from google.colab import files\n","files.download(output_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"6tbhlRSr8cOD","executionInfo":{"status":"ok","timestamp":1752278820097,"user_tz":-330,"elapsed":69,"user":{"displayName":"Aryan Singh","userId":"11476742217229803552"}},"outputId":"b1bf1dc1-bd88-4641-c671-6d9107e7902d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Graph successfully saved to: /content/twitter_network.gexf\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_77b12923-e513-4650-b5f1-1b02a64342f5\", \"twitter_network.gexf\", 8212)"]},"metadata":{}}]}]}